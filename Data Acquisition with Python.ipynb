{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "All data problems begin with a question and end with a narrative construct that provides a clear answer. From there, the next step is getting your data. As a Data Scientist, you'll spend an incredible amount of time and skills on acquiring, prepping, cleaning, and normalizing your data. In this tutorial, we'll review some of the best tools used in the rhelm of data acquisition. \n",
    "\n",
    "But first, let's go into the differences between Data Acquisition, Preparation, and Cleaning. \n",
    "\n",
    "### Data Acquisition\n",
    "\n",
    "Data Acquisition is the process of getting your data, hence the term <i>acquisition</i>. Data doesn't come out of nowhere, so the very first step of any data science problem is going to be getting the data in the first place. \n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "Once you have the data, it might not be in the best format to work with. You might have scraped a bunch of data from a website, but need it in the form of a dataframe to work with it in an easier manner. This process is called data preparation - preparing your data in a format that's easiest to form with.\n",
    "\n",
    "### Data Cleaning\n",
    "\n",
    "Once your data is being stored or handled in a proper manner, that might still not be enough. You might have missing values or values that need normalizing. These inconsistencies that you fix before analysis refers to data cleaning. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading, Writing, and Handling Data Files\n",
    "\n",
    "The simplest way of acquiring data is downloading a file - either from a website, straight from your desktop, or elsewhere. Once the data is downloaded, you'll open the files for reading and possible writing. \n",
    "\n",
    "### CSV files\n",
    "\n",
    "Very often, you'll have to work with CSV files. A csv file is a comma-separated values file stores tabular data in plain text. \n",
    "\n",
    "In the following examples, we'll be working with NBA data, which you can download from [here](https://github.com/ByteAcademyCo/data-acq/blob/master/nba.csv).\n",
    "\n",
    "#### CSV \n",
    "\n",
    "Python has a csv module, which you can utilize to work with CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, with the following 4 lines, you can print each row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('nba.csv', 'rt') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fairly straightforward, but let's see how else we can accomplish this. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas\n",
    "\n",
    "Alternatively, you can use Pandas. Pandas is great for working with CSV files because it handles DataFrames. \n",
    "\n",
    "We begin by importing the needed libraries: pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use pandas to read the CSV file and show the first few rows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "task_data = pd.read_csv(\"nba.csv\")\n",
    "task_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, by using pandas, we're able to fasten the process of viewing our data, as well as view it in a much more readable format. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### R Programming\n",
    "\n",
    "We've just gone through how to read CSV files in Python. But how do you do this in R? Pretty simply, actually. R has built in functions to handle CSV files, so you don't even have to use a library to accomplish what we just did with Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data <- read.csv(\"nba.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON\n",
    "\n",
    "Because HTTP is a protocol for transferring text, the data you request through a web API (which we'll go through soon enough) needs to be serialized into a string format, usually in JavaScript Object Notation (JSON). JavaScript objects look quite similar to Python dicts, which makes their string representations easy to interpret:\n",
    "\n",
    "```\n",
    "{ \n",
    " \"name\" : \"Lesley Cordero\",\n",
    " \"job\" : \"Data Scientist\",\n",
    " \"topics\" : [ \"data\", \"science\", \"data science\"] \n",
    "}\n",
    "```\n",
    "\n",
    "Python has a module sepcifically for working with JSON, called `json`, which we can use as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "serialized = \"\"\" { \n",
    " \"name\" : \"Lesley Cordero\",\n",
    " \"job\" : \"Data Scientist\",\n",
    " \"topics\" : [ \"data\", \"science\", \"data science\"] \n",
    "} \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we parse the JSON to create a Python dict, using the json module: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "deserialized = json.loads(serialized)\n",
    "print(deserialized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(type(deserialized))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APIs\n",
    "\n",
    "There are several ways to extract information from the web. Use of APIs, Application Program Interfaces, is probably the best way to extract data from a website. APIs are especially great if your data is constantly changing. Many websites have public APIs providing data feeds via JSON or some other format. \n",
    "\n",
    "There are a number of ways to access these APIs from Python. In order to get the data, we make a request to a webserver, hence an easy way is to use the `requests` package. \n",
    "\n",
    "### GET request\n",
    "\n",
    "There are many different types of requests. The most simplest is a GET request. GET requests are used to retrieve your data. In Python, you can make a get request to get the latest position of the international space station from the `OpenNotify` API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "response = requests.get(\"http://api.open-notify.org/iss-now.json\")\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which brings us to status codes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Status Codes\n",
    "\n",
    "What we just printed was a status code of `200`. Status codes are returned with every request made to a web server and indicate what happened with a request. The following are the most common types of status codes:\n",
    "\n",
    "- `200` - everything worked as planned!\n",
    "- `301` - the server is redirecting you to anotehr endpoint (domain).\n",
    "- `400` - it means you made a bad request by not sending the right data or some other error.\n",
    "- `401` - you're not authenticated, which means you don't have access to the server.\n",
    "- `403` - this means access is forbidden. \n",
    "- `404` - whatever you tried to access wasn't found. \n",
    "\n",
    "Notice that if we try to access something that doesn't exist, we'll get a `404` error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = requests.get(\"http://api.open-notify.org/iss-pass\")\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a get request where the status code returned is `404`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = requests.get(\"http://api.open-notify.org/iss-pass.json\")\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like we mentioned before, this indicated a bad request. This is because it requires two parameters, as you can see [here](http://open-notify.org/Open-Notify-API/ISS-Pass-Times/). \n",
    "\n",
    "We set these with an optional `params` variable. You can opt to make a dictionary and then pass it into the `requests.get` function, like follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {\"lat\": 40.71, \"lon\": -74}\n",
    "response = requests.get(\"http://api.open-notify.org/iss-pass.json\", params=parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can skip the variable portion with the following instead: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response = requests.get(\"http://api.open-notify.org/iss-pass.json?lat=40.71&lon=-74\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty messy, but luckily, we can clean this up into JSON with:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = response.json()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping\n",
    "\n",
    "Web Scraping tools are specifically developed for extracting information from websites. Web Scraping mostly focuses on the transformation of unstructured data (HTML format) on the web into structured data (database or spreadsheet).\n",
    "\n",
    "### HTML\n",
    "\n",
    "While performing web scraping, we deal with html tags. Thus, we must have good understanding of them. Below is the basic syntax of HTML:\n",
    "\n",
    "``` html\n",
    "<!DOCTYPE html> \n",
    "<html>\n",
    "\t<body>\n",
    "\t\t<h1> First Heading </h1>\n",
    "\t\t<p> First Paragraph </p>\n",
    "\t</body>\n",
    "</html>\n",
    "```\n",
    "Let's break down each of these tags:\n",
    "\n",
    "1. `<!DOCTYPE html>`: This is the initial HTML declaration.\n",
    "2. `<html>`: The HTML document is going to be contained within this tag.\n",
    "3. `<body>`: This is where the visible portion of the HTML document is between. \n",
    "4. `<h1>`: This is an HTML heading.\n",
    "5. `<p>`: HTML paragraphs are defined here. \n",
    "\n",
    "We've also got the following tags:\n",
    "\n",
    "- `<a>`: These always define HTML links, such as with \n",
    "``` HTML\n",
    "<a href=\"http://byteacademy.co\">This is Byte Academy's website!</a>\n",
    "```\n",
    "- `<table>`: HTML tables are defined with this tag, such as:\n",
    "*Note that the `<tr>`are rows and `<td>` defines columns. \n",
    "``` HTML\n",
    "<table style=\"width:100%\">\n",
    "\t<tr>\n",
    "\t\t<td>Lesley</td>\n",
    "\t\t<td>Cordero</td>\n",
    "\t\t<td>24</td>\n",
    "\t</tr>\n",
    "\t<tr>\n",
    "\t\t<td>Helen</td>\n",
    "\t\t<td>Chen</td>\n",
    "\t\t<td>22</td>\n",
    "\t</tr>\n",
    "</table>\n",
    "```\n",
    "\n",
    "This will yield the following:\n",
    "\n",
    "```\n",
    "Lesley\t\tCordero\t\t24\n",
    "Helen\t\tChen\t\t22\n",
    "```\n",
    "- `<li>` initializes the beginning of a list. `<ul>` and `<ol>` each define whether it's an unordered list or an ordered list. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BeautifulSoup\n",
    "\n",
    "BeautifulSoup is used to parse the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "wiki = \"https://en.wikipedia.org/wiki/List_of_states_and_territories_of_the_United_States\"\n",
    "page = requests.get(wiki)\n",
    "html = page.content\n",
    "\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's explore this webpage! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This outputs the title of the wikipedia page: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(soup.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The string version of this can be obtained with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(soup.title.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `soup.a`, we can output the links under the `<a>` tag. We get the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(soup.a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But this only allows us to have one output. If we want to extract all the links within `<a>`, we will use `find_all()`, as the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_links = soup.find_all(\"a\")\n",
    "for i in all_links:\n",
    "    print(i.get(\"href\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're looking for the capital of each state, let's use `find_all` to retrieve the table tags:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_tables = soup.find_all('table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to identify the right table. We filter this by figuring out what the attribute class name is. In chrome, you can check the class name by right click on the table of web page. Then you click \"Inspect\" and copy the class name. You also go through the output of above command find the class name of right table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "right_table = soup.find('table', class_='wikitable sortable plainrowheaders')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now going to store the data from the website. We'll grab the first few columns, so we'll initialize a list for each of these here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A = []\n",
    "B = []\n",
    "C = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, is to actually grab the needed data and add it to each list. We iterate through the scraped data, row by row:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row in right_table.findAll(\"tr\"):\n",
    "    cells = row.findAll('td')\n",
    "    states = row.findAll('th') \n",
    "    if len(cells) == 9 or len(cells) == 8: \n",
    "        A.append(cells[0].find(text=True))\n",
    "        B.append(states[0].find(text=True))\n",
    "        C.append(cells[1].find(text=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we actually create the DataFrame with pandas: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(A, columns=['Number'])\n",
    "df['State/UT'] = B\n",
    "df['Admin_Capital'] = C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Web Scraping\n",
    "\n",
    "\n",
    "### Sitemaps\n",
    "\n",
    "The Sitemaps protocol allows a webmaster to inform search engines about URLs on a website that are available for crawling. A Sitemap is an XML file that lists the URLs for a site. It allows webmasters to include additional information about each URL: when it was last updated, how often it changes, and how important it is in relation to other URLs in the site. This allows search engines to crawl the site more intelligently. Sitemaps are a URL inclusion protocol and complement robots.txt, a URL exclusion protocol.\n",
    "\n",
    "An example of what a sample XML sitemap might look like is:\n",
    "\n",
    "``` \n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "\n",
    "<urlset xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\">\n",
    "\n",
    "   <url>\n",
    "\n",
    "      <loc>http://www.example.com/</loc>\n",
    "\n",
    "      <lastmod>2005-01-01</lastmod>\n",
    "\n",
    "      <changefreq>monthly</changefreq>\n",
    "\n",
    "      <priority>0.8</priority>\n",
    "\n",
    "   </url>\n",
    "\n",
    "</urlset> \n",
    "```\n",
    "\n",
    "### Estimating Website Size\n",
    "\n",
    "The size of the website will affect how you crawl it. If the website is just a few hundred URLs, such as our example website, efficiency is not important. However, if the website has over a million web pages, downloading each sequentially would take months. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions\n",
    "\n",
    "A regular expression is a sequence of characters that define a string.\n",
    "\n",
    "#### Simplest Form\n",
    "\n",
    "The simplest form of a regular expression is a sequence of characters contained within <b>two backslashes</b>. For example, <i>python</i> would be  \n",
    "\n",
    "``` \n",
    "\\python\n",
    "```\n",
    "\n",
    "#### Case Sensitivity\n",
    "\n",
    "Regular Expressions are <b>case sensitive</b>, which means \n",
    "\n",
    "``` \n",
    "\\p and \\P\n",
    "```\n",
    "are distinguishable from eachother. This means <i>python</i> and <i>Python</i> would have to be represented differently, as follows: \n",
    "\n",
    "``` \n",
    "\\python and \\Python\n",
    "```\n",
    "\n",
    "We can check these are different by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "re1 = re.compile('[0-9]') # regular expression\n",
    "print(bool(re1.match('')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Disjunctions\n",
    "\n",
    "If you want a regular expression to represent both <i>python</i> and <i>Python</i>, however, you can use <b>brackets</b> or the <b>pipe</b> symbol as the disjunction of the two forms. For example, \n",
    "``` \n",
    "[Pp]ython or \\Python|python\n",
    "```\n",
    "could represent either <i>python</i> or <i>Python</i>. Likewise, \n",
    "\n",
    "``` \n",
    "[0123456789]\n",
    "```\n",
    "would represent a single integer digit. The pipe symbols are typically used for interchangable strings, such as in the following example:\n",
    "\n",
    "```\n",
    "\\dog|cat\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranges\n",
    "\n",
    "If we want a regular expression to express the disjunction of a range of characters, we can use a <b>dash</b>. For example, instead of the previous example, we can write \n",
    "\n",
    "``` \n",
    "[0-9]\n",
    "```\n",
    "Similarly, we can represent all characters of the alphabet with \n",
    "\n",
    "``` \n",
    "[a-z]\n",
    "```\n",
    "\n",
    "#### Exclusions\n",
    "\n",
    "Brackets can also be used to represent what an expression <b>cannot</b> be if you combine it with the <b>caret</b> sign. For example, the expression \n",
    "\n",
    "``` \n",
    "[^p]\n",
    "```\n",
    "represents any character, special characters included, but p."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question Marks \n",
    "\n",
    "Question marks can be used to represent the expressions containing zero or one instances of the previous character. For example, \n",
    "\n",
    "``` \n",
    "<i>\\colou?r\n",
    "```\n",
    "represents either <i>color</i> or <i>colour</i>. Question marks are often used in cases of plurality. For example, \n",
    "\n",
    "``` \n",
    "<i>\\computers?\n",
    "```\n",
    "can be either <i>computers</i> or <i>computer</i>. If you want to extend this to more than one character, you can put the simple sequence within parenthesis, like this:\n",
    "\n",
    "```\n",
    "\\Feb(ruary)?\n",
    "```\n",
    "This would evaluate to either <i>February</i> or <i>Feb</i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kleene Star\n",
    "\n",
    "To represent the expressions containing zero or <b>more</b> instances of the previous character, we use an <b>asterisk</b> as the kleene star. To represent the set of strings containing <i>a, ab, abb, abbb, ...</i>, the following regular expression would be used:  \n",
    "```\n",
    "\\ab*\n",
    "```\n",
    "\n",
    "#### Wildcards\n",
    "\n",
    "Wildcards are used to represent the possibility of any character and symbolized with a <b>period</b>. For example, \n",
    "\n",
    "```\n",
    "\\beg.n\n",
    "```\n",
    "From this regular expression, the strings <i>begun, begin, began,</i> etc., can be generated. \n",
    "\n",
    "#### Kleene+\n",
    "\n",
    "To represent the expressions containing at <b>least</b> one or more instances of the previous character, we use a <b>plus</b> sign. To represent the set of strings containing <i>ab, abb, abbb, ...</i>, the following regular expression would be used:  \n",
    "\n",
    "```\n",
    "\\ab+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### REs & BeautifulSoup \n",
    "\n",
    "If the previous section on regular expressions seemed a little disjointed, hereâ€™s where it all ties together. BeautifulSoup and regular expressions go hand in hand when it comes to scraping the Web. In fact, most functions that take in a string argument will also take in a regular expression as well. \n",
    "\n",
    "Looking at [this](http://www.pythonscraping.com/pages/page3.html) webpage, we'll see that there are product images of the following form:\n",
    "\n",
    "``` HTML\n",
    "<img src=\"../img/gifts/img3.jpg\">\n",
    "```\n",
    "If we wanted to grab URLs to all of the product images, it might seem fairly straightforward: just grab all the image tags using `.findAll(\"img\")`. Unfortunately, thereâ€™s a problem. There are â€œextraâ€ images (i.e, logos), often have hidden images, and blank images used for spacing and aligning elements, and other random image tags you might not be aware of.\n",
    "\n",
    "The solution is to look for another approach. In this case, we can look at the file path of the product images. You've seen this part before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "html = urlopen(\"http://www.pythonscraping.com/pages/page3.html\")\n",
    "bsObj = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This prints out only the relative image paths that start with ../img/gifts/img and end in `.jpg`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = bsObj.findAll(\"img\", {\"src\":re.compile(\"\\.\\.\\/img\\/gifts/img.*\\.jpg\")})\n",
    "for image in images:\n",
    "    print(image[\"src\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A regular expression can be inserted as any argument in a BeautifulSoup expression, allowing you a great deal of flexibility in finding target elements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda Expressions\n",
    "\n",
    "Recall, a lambda expression is a function that is passed into another function as a variable. Instead of defining a function as f(x, y), you may define a function as f(g(x), y), or even f(g(x), h(x)).\n",
    "\n",
    "BeautifulSoup allows us to pass certain types of functions as parameters into the findAll function. The only restriction is that these functions must take a tag object as an argument and return a boolean. Every tag object that BeautifulSoup encounters is evaluated in this function, and tags that evaluate to â€œtrueâ€ are returned while the rest are discarded.\n",
    "\n",
    "For example, the following retrieves all tags that have exactly two attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bsObj.findAll(lambda tag: len(tag.attrs) == 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using lambda functions in BeautifulSoup, selectors can act as a great substitute for writing a regular expression, if youâ€™re comfortable with writing a little code.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
